\section{Design Choices}

We discuss three main overarching design choices that one has to make when integrating ML components with DBMSs: 1) integration mode, 2)  learning source and 3) choice of ML paradigm.


\subsection{Integration Mode}
We found that there are two main engineering approaches for integrating ML components into DBMSs: external vs. internal integration ~\cite{pavlo2019external}.

\vspace{2mm}
\noindent \textbf{External Integration.} Modern DBMSs are complex systems and they allow human database administrators to control the query execution performance by (1) optimizing the physical database design, (2) providing query optimization hints, (3) knob tuning, and (4) resource provisioning.
They also provide information about the system such as resource usage, query traces, and performance metrics.
Under this context, the focus of externally integrated ML components is to provide recommendations to human database administrators or replace them and automatically perform the tasks using the standard configuration endpoints provided by the DBMS.

An enterprise-grade DBMS typically requires decades of highly advanced software development efforts and thus there is huge resistance among DBMS developers to integrate new components that require significant architectural changes.
External integration keeps the ML components outside the critical path of a DBMS and still provides a value.
For this reason, most of the systems surveyed in this paper fall into this category (see Figure 1) and some have even been successfully adopted by several enterprise DBMSs.

However, external integration also faces several limitations.
First developing multiple external components that operate on different sub-problems may lead to interference among the decisions taken by those systems.
For example, assume an external query optimization component that hints a specific query plan to DBMS assuming the absence of a particular index.
At the same time assume there is another physical database design component that decides to create this index which renders the chosen evaluation plan become sub-optimal.
Avoiding this kind of interference requires coordination among different components, which is difficult to implement in external components.
Second, external components for knob tuning and resource provisioning take an iterative approach where they try out several different settings before picking the best option.
Existing DBMSs are not optimized for such rapid experimentation and hence require system downtimes or restarts for the configurations to take effect.
This significantly increases the time required for knob tuning by an ML component.
Finally, the system information metrics provided by the DBMSs are primarily intended to be consumed by human database administrators for diagnosing performance bottlenecks.
Thus they can be too high-level for ML components to learn from.

\vspace{2mm}
\noindent \textbf{Internal Integration.} 
Internal integration of ML components tries to mitigate much of the above-mentioned limitations by changing the DBMS architecture to treat ML components as first-class components.
As a result, ML components get more access to the low-level information and more fine-grained control to the DBMS.
Coherence among the decisions taken by multiple ML components inside a DBMS can be achieved by having a centralized coordinator that takes suggested actions from different ML components and execute them only if they don't interfere with other decisions.
However, internal integration requires tight coupling between the components inside a DBMS and can pose query execution performance degradation when training the ML models.
Hence, they are mostly applicable to new data system developments that are being developed from scratch (also called greenfield systems).
% Several vision systems have already proposed novel learned DBMS architectures.

NoisePage~\cite{noisepage} is an in-memory hybrid transactional and analytic processing DBMS, which can automatically optimize query execution without a human database administrator.
It focusses on knob tuning, resource provisioning, and physical database design optimization.
NoisePage has a modular architecture optimized for efficient offline training data collection by ML components.
Training data for each module (e.g., transaction manager) can be obtained in isolation without the need of going through the entire DBMS execution path.
These offline collected data is then combined with the data collected through online query execution to learn ML models.
The ML pipeline in NoisePage has three main phases: 1) modeling, 2) planning, 3) deployment.
In the modeling stage, it builds models to predict the future query workload and models to predict the behavior of system components under different configuration values.
In the planning stage, it uses reinforcement learning to pick actions based on the models trained in the modeling phase, instead of interacting with the actual system.
Finally, in the deployment phase, the chosen actions are applied and the observed performance metrics are later fed back to modeling and planning models to improve their performance.

SageDB~\cite{sagedb} is another system that proposes a novel DBMS architecture that uses ML models combined with program synthesis techniques to generate internal system components like data structures and algorithms.
To balance the training time vs. accuracy it proposes using multiple ML models each specialized for a particular task.
ML models in SageDB are optimized to capture the empirical data distribution of the data and not optimized for the ability to generalize to unseen data.
These synthesized systems components are used for optimizing data access (e.g., indices), query optimization (e.g., cardinality estimation), and query execution (e.g., sorting).


\subsection{Learning Source} 
The main goal of using ML for DBMS components is to improve the performance of the system for future query workloads.
Thus one way for adopting ML methods is to learn from past or current (in the case of reinforcement learning) queries.
But the performance of the queries is dependant on the state of the underlying data in the DBMS.
Hence, in some cases, it is also possible to achieve the same goal by learning directly from the data.
% Next we explain some of the popular systems which adopts these two approaches and compare their pros and cons.

\vspace{2mm}
\noindent \textbf{Learning from Queries.} As shown in Figure 1, learning from query workloads is the most widely used approach for integrating ML into DBMS across all components.
Learning from queries enables the ML models to learn a narrow scoped problem which is much easier to model/learn and hence improve the overall performance of the system.

However, this approach faces three main challenges.
First, collecting training data for this approach can be expensive as each query needs to be executed on large databases.
Second, it does not generalize well for unseen workload queries and causes significant performance degradation at execution time.
Third, changes in the workload patterns or underlying data require capturing new training data and expensive retraining which can cause system downtime.

\vspace{2mm}
\noindent \textbf{Learning from Data.} More recently several systems have been proposed that learn from DB-resident data, instead of query workloads.
These systems train models to learn the empirical data distribution of the data and use them to improve DBMS performance.
For example, DBEst~\cite{dbest} and DeepDB~\cite{deepdb} uses probability distribution models to answer AQP queries.
Naru~\cite{naru} and DeepDB~\cite{deepdb} use joint probability distribution models in the relational engine to optimize cardinality estimates.
Empirical data distribution models are also the main building block in learned data structures and algorithms such as Learned Index~\cite{learnedindex}, FITing-Tree~\cite{fitingtree}, and XIndex~\cite{xindex}.

ML models trained using data can be reused despite changes in the workload pattern and are also more robust to small changes in the data.
More importantly, \cite{deepdb} has shown that the same probability distribution model can be used in multiple tasks such as AQP and cardinality estimation, reducing the total training time required.
However, accurately capturing the joint probability distribution of a relational dataset with multiple tables is a complex learning task that requires models with high learning capacity and training time.
Furthermore, not all DBMS components can be purely learned from data (e.g., execution engine knob tuning).

\vspace{2mm}
\noindent \textbf{Hybrid Methods.} While most of the existing systems fall into one of the above two approaches, it would be interesting to explore the possibility of combining both approaches.
Some early work in this regard has been proposed in the XIndex~\cite{xindex} system.
XIndex is a learned index structure that replaces B-tree indices.
It does so by learning the empirical cumulative distribution function of the keys of the data.
During training, the goal is to minimize the maximum error made by the model for predicting the position of a key.
But the performance of a particular workload will be dominated by the errors made by the model on the keys that are frequently used in the workload.
Hence, XIndex performs the second phase of learning where it dynamically further minimizes the error on the most frequently used keys.


\subsection{Choice of the ML Paradigm}
We cover the use of different ML model and learning paradigms for integrating ML into DBMSs.

\subsubsection{ML Model Family}
We observed two prominent families of ML models: 1) deep learning and 2) classical ML.

\vspace{2mm}
\noindent \textbf{Deep Learning.} It should be noted that much of the recent renaissance in applying ML methods for DBMS internals, and also systems in general, has to be credited to the recent advancements in deep learning.
Deep learning models have high model capacities and hence can learn highly complex data distributions.
They have shown superior performance in hard tasks such as in natural language processing (NLP).
As a result, the same deep NLP models have been used in systems like SpeakQL~\cite{speakql}, SeqSQL~\cite{seq2sql}, and SQLNet~\cite{sqlnet} to provide spoken and/or natural language interfaces for DBMSs.
Naru ~\cite{naru} uses a transformer-based deep learning model to learn the joint probability distribution of the data for cardinality estimation.
Neo~\cite{neo}, DQM~\cite{dqm}, and iBTune~\cite{ibtune} also use deep learning models for query optimization (QEP generation), physical database design (materialization optimization), and knob tuning (buffer size tuning), respectively.

However, the high model capacity of deep learning models and their ability to learn highly accurate models comes at a cost.
First, these models require significantly large amounts of training data without which the models will start to overfit. In many DBMS components, collecting large amounts of training data is expensive as the queries have to be executed potentially on large databases.
Second, deep learning models are highly compute-intensive which can require few hours to a few days of training even when using expensive hardware accelerators such as GPUs. This can cause degradation of DBMS query execution performance.
Furthermore, typically deep learning inference times are in few hundreds of milliseconds and do not match with the performance requirements of the DBMS components such as cardinality estimators and indices which have to be in the order of few milliseconds.
Finally, the explainability/debuggability of deep learning models is still an active area of research and there is minimal understanding of the internal workings of them.
As a result, while deep learning-based methods have shown promising accuracy results, they are not yet widely integrated into enterprise DBMSs due to the above open challenges.


\vspace{2mm}
\noindent \textbf{Classical ML.} We found that classical ML-based methods are widely used to integrate ML into DBMS components and some of them (e.g., LEO~\cite{leo}, CardLearner~\cite{cardlearner}, AutoAdmin~\cite{autoadmin}, and VerdictDB~\cite{verdict}) have been even integrated in the enterprise systems.
Classical ML methods overcome much of the limitations of deep learning methods: they are less compute-intensive to train, require much less training data, have faster inference times, and generate much easy to explain predictions.
However, most classical ML models are known to have fewer model capacities and thus less predictive power compared to deep learning models. Thus their accuracy can be lower.


\subsubsection{Learning Paradigm}
We explain the use of three learning paradigms that systems have used to integrate ML into DBMSs: 1) supervised learning, 2) reinforcement learning, and 3) unsupervised learning.

\vspace{2mm}
\noindent \textbf{Supervised Learning.} Out of the systems we surveyed supervised learning approach is the most widely used learning paradigm (see Figure 1).
The training data required for supervised learning is collected either beforehand or continuously during query execution.
Seq2SQL~\cite{seq2sql} and SQLNet~\cite{sqlnet} use a large manually generated dataset of SQL and natural language query pairs.
Most other systems (e.g., CardLearner~\cite{cardlearner}, QPPNet~\cite{qppnet}, Naru~\cite{naru}, AutoAdmin~\cite{autoadmin, autoadmin_2}, and Learned Index ~\cite{learnedindex}) preform initial training from previously collected training data and then perform periodic retraining as new data becomes available or the query workload or the data significantly change.

In some cases, collecting training data can be expensive as it requires executing a large number of queries potentially on large databases.
However, the prevalence of cloud databases has mitigated these issues as cloud operators have access to large amounts of query execution traces from many tenants.

\vspace{2mm}
\noindent \textbf{Reinforcement Learning.} Reinforcement learning (RL) methods are particularly applicable when a system component has to make a series of decisions and the reward of each decision is not directly observable.
Thus, RL methods have been used in tasks including QEP generation (e.g., SkinnerDB~\cite{skinnerdb}, ReJoin~\cite{rejoin}, Neo~\cite{neo}), materialization optimization (e.g., DQM~\cite{dqm}) and scheduling (e.g., Bandit~\cite{bandit}).

RL methods perform on the job training and collect as the DBMS execute queries.
Initially, they may generate worse results as the models have not converged yet.
To overcome this, one could use a bootstrapping strategy called  \textit{learn by demonstration} where the existing DBMS component is used to generate initial training data for the RL model to bootstrap.
After this initial training, the RL model will continue to learn on the job and become better than the existing DBMS component.
For example, Neo~\cite{neo} showed the feasibility of building an RL-based learned query optimizer which surpasses the PostgreSQL DBMS query optimizer, even though the RL model was initially bootstrapped using it.


\vspace{2mm}
\noindent \textbf{Unsupervised Learning.} We found that unsupervised learning techniques are widely used for DBMS knob tuning in systems like iTuned~\cite{ituned}, OtterTune~\cite{ottertune}, and QB5000~\cite{qb5000}.
One such popular technique is to reduce the number of different queries by performing clustering based on query templates.
While DBMS may encounter a large number of different queries, most of them are different parameterizations of the same query template.
Thus, by reducing the queries into query templates the complexity for the ML model can be significantly reduced.
iTuned~\cite{ituned} and OtterTune~\cite{ottertune} also use Gaussian mixture models, another unsupervised learning method, to model the DBMS performance corresponding to different systems configurations settings.