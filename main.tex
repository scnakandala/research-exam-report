%!TEX root = <main.tex>
\documentclass[sigconf, 10pt]{acmart}
% \documentclass[acmsmall]{acmart}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} 
\pagenumbering{arabic}
\pagestyle{plain}

\usepackage[font=small,labelfont=bf]{caption}
\usepackage{graphicx,xspace,verbatim,comment}
\usepackage{hyperref,array,color,balance,multirow}
\usepackage{balance,float,url,amsfonts,alltt}
\usepackage{mathtools,rotating,amsmath,amssymb}
\usepackage{color,ifpdf,fancyvrb}
\usepackage{etoolbox,listings,subcaption}
\usepackage{bigstrut,morefloats,pbox}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{bm}

\newcommand{\thanya}{\textcolor{red}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\red}{\textcolor{red}}
\newcommand{\titlename}{Machine Learning for Database Management Systems}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newenvironment{packeditems}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packedenums}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}\sloppy
\title{\titlename}

\author{Supun Nakandala}
\affiliation{
  \institution{University of California, San Diego}
}
\email{snakanda@eng.ucsd.edu}

\maketitle
\begin{abstract}
\end{abstract}

\input{introduction.tex}
\input{background.tex}
\input{parser.tex}
\input{relational_engine.tex}
\input{execution_engine.tex}
\input{design_choices.tex}
\input{challenges.tex}

% \section{Excerpts from Papers}

% Seattle Report \cite{seattle-report} Recent advances in ML have inspired our community to reflect on how
% some of hard data engine problems could use ML to advance the state of the art. The most obvious such problems are in auto tuning.
% For example, we can systematically replace ``magic numbers'' in database systems with data-driven learning models and use them to auto-tune system configurations.
% ML also provides new hope for progress in query optimization, which has seen only minor improvements in the last two decades.
% Although in principle almost any component can potentially be improved with ML, answers to some key questions are prerequisites for success, such as availability of training data, a well-thought software engineering pipeline to support an ML component (debuggability is notoriously hard), and availability of the guard-rails so that when test data or test queries deviate from the training data and training queries, the system degrades gracefully.

\section{Conclusions}
ML is revolutionizing many fields by providing a flexible framework for programming the unprogrammable.
In this paper, we survey the existing landscape of applying ML techniques for optimizing DBMS internal components that have hitherto relied on heuristics and/or scope restrictions to implement the unprogrammable.
We also identify several overarching design choices one has to make when incorporating ML into DBMS components.
While there are some major accomplishments, the field is still in its infancy and we identify three key open challenges.


% \pagebreak

\bibliographystyle{unsrt}
\balance
\bibliography{main}

\end{document}